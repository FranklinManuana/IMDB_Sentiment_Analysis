{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213157ec",
   "metadata": {},
   "source": [
    "### Phase 1: Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5eb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# import dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# import libraries to clean text\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# import sklearn and transformers\n",
    "# Traditional ML\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# BERT-untrained\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Bert-trained\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52926f",
   "metadata": {},
   "source": [
    "### Phase 2: Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551820a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44055497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take small subset to test models\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46158a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of \n",
    "def plot_eda(review):\n",
    "    # Class distribution\n",
    "    sns.countplot(x='label', data=review)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "     # Review length distribution\n",
    "    small_train_dataset['text_length'] = small_train_dataset['text'].apply(lambda x: len(x))\n",
    "    sns.histplot(small_eval_dataset['text_length'], bins=50)\n",
    "    plt.title('Review Length Distribution')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d34ab",
   "metadata": {},
   "source": [
    "### Phase 3: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional ML\n",
    "\n",
    "# function to remove html tags\n",
    "def remove_html_tags(review):\n",
    "    clean_text = re.sub('<.*?>', '', review)\n",
    "    return clean_text\n",
    "\n",
    "# function to remove punctuation\n",
    "def remove_punctuation(review):\n",
    "    no_punc = review.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    return no_punc\n",
    "\n",
    "\n",
    "# function for TF_IDF\n",
    "def TF_IDF(review):\n",
    "    tokens = review.lower()\n",
    "    vectorizer = TfidfVectorizer(max_features = 10000)\n",
    "    return vectorizer.fit_transform(review)\n",
    "\n",
    "\n",
    "# make a preprocessing function that cleans reviews and performs TF-IDF\n",
    "def preprocessing(review):\n",
    "    preprocess = review['text'].apply(lambda x: remove_html_tags(x))\n",
    "    preprocess = preprocess.apply(lambda x: remove_punctuation(x))\n",
    "    preprocess= preprocess.apply(lambda x: TF_IDF(x))\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BERT-untrained preprocessing\n",
    "def tokenize_untrained():\n",
    "    return tokenizer(small_train_dataset['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# DistilBERT-trained preprocessing\n",
    "def tokenize_trained():\n",
    "    return tokenizer(small_train_dataset['text'], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2f791",
   "metadata": {},
   "source": [
    "### Phase 4: Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ec0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untrained\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"./results_scratch\"\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, # model to be trained\n",
    "    args = training_arguments, # trainer arguments\n",
    "    train_dataset = small_train_dataset, # training set\n",
    "    eval_dataset = small_eval_dataset # evaluation set\n",
    ")\n",
    "\n",
    "# Trained\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"./results_finetuned\"\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, # model to be trained\n",
    "    args = training_arguments, # trainer arguments\n",
    "    train_dataset = small_train_dataset, # training set\n",
    "    eval_dataset = small_eval_dataset # evaluation set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11083f49",
   "metadata": {},
   "source": [
    "### Phase 5: Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12216ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional ML\n",
    "def trad_ML(X_train, y_train):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "# untrained model\n",
    "def untrained_bert(dataset_train, dataset_test):\n",
    "    config = DistilBertConfig(num_labels=2) # confiugre untrained model 2 labels\n",
    "    model = DistilBertForSequenceClassification(config) # add configuration to untrained model\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\") # Tokenization method\n",
    "\n",
    "    def tokenization(dataset):\n",
    "        return tokenizer(dataset[\"text\"], padding = \"max_length\", truncation=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # setup the training Parameters\n",
    "    training_arguments = TrainingArguments(\n",
    "    output_dir = \"./results_scratch\"\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model, # model to be trained\n",
    "        args = training_arguments, # trainer arguments\n",
    "        train_dataset = small_train_dataset, # training set\n",
    "        eval_dataset = small_eval_dataset # evaluation set\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "    \n",
    "# pretrained model\n",
    "def trained_bert(dataset_train, dataset_test):\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    \n",
    "    # setup training Parameters\n",
    "    training_arguments = TrainingArguments(\n",
    "    output_dir = \"./results_finetuned\"\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model, # model to be trained\n",
    "        args = training_arguments, # trainer arguments\n",
    "        train_dataset = small_train_dataset, # training set\n",
    "        eval_dataset = small_eval_dataset # evaluation set\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c682ae2",
   "metadata": {},
   "source": [
    "### Phase 6: Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
